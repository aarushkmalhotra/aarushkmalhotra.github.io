[
  {
    "id": "vernato",
    "name": "Vernato",
    "tagline": "Learn to speak better",
    "tagline": "Learn to speak better",
    "description": "I discovered Duolingo's speech exercises have a major flaw - you can literally speak gibberish and still pass. That's when I realized language learners desperately need real pronunciation feedback, not just token speech recognition that accepts anything.\n\nVernato solves this by providing genuine pronunciation scoring with detailed breakdowns of accuracy, fluency, and completeness. The app works like having a pronunciation coach in your pocket, giving you specific, actionable feedback instead of vague suggestions to 'try again.'\n\nWhat makes Vernato special is its regional variant support. Through testing with friends living across different countries, I learned that the same language has different pronunciation norms - English in London sounds different from English in New York. Vernato accounts for these cultural contexts so you're not penalized for pronouncing something correctly in your region.",
    "role": "I handle everything from product strategy to full-stack development. This includes designing the core pronunciation scoring algorithm, building the web application, setting up cloud infrastructure, and fine-tuning the user experience. The most challenging part is making feedback feel encouraging rather than discouraging - pronunciation practice already makes people nervous.",
    "problem": "Language learning apps like Duolingo have speech exercises that accept gibberish as correct pronunciation. This gives learners false confidence while missing the crucial skill of speaking correctly and confidently. Reading and writing are important, but being able to speak a language properly is equally vital for real communication.",
    "approach": "I started by defining clear metrics for pronunciation quality - accuracy, fluency, and completeness. The first version was simple: record audio, analyze it, show a score. I tested extensively with friends living abroad to understand regional pronunciation differences and built support for different cultural contexts of the same language.",
    "challenges": "The main challenge was psychological - providing meaningful feedback without crushing confidence. I also had to handle messy real-world audio from different devices and environments. Performance was crucial too - users expect instant feedback, so I optimized everything from audio processing to response times.",
    "keyFeatures": [
      "PronScoreâ„¢ model for comprehensive feedback",
      "Actionable insights on accuracy, fluency, and completeness",
      "Designed for individuals, educators, and organizations",
      "Engaging and measurable pronunciation training"
    ],
    "techStack": "TypeScript, React, Next.js, Azure, Google Cloud, Firebase, Vercel, Tailwind CSS, Git",
    "keywords": ["AI & Machine Learning", "Web Development", "Creative Tech"],
    "outcomes": "The r/languagelearning community on Reddit embraced Vernato enthusiastically, providing valuable feedback that shaped the product. Friends living abroad tested various regional pronunciation variants, helping me validate the cultural context features. The app provides feedback in under seven seconds and gives users the targeted pronunciation guidance that traditional language learning apps lack.",
    "repoUrl": null,
    "demoUrl": "https://www.vernato.org",
    "images": ["vernato-1"],
    "startDate": "2025-06-01",
    "endDate": null,
    "type": "project",
     "theme": {
      "primary": "hsl(196, 75%, 52%)",
      "secondary": "hsl(45, 100%, 55%)"
    }
  },
  {
    "id": "emty",
    "name": "EMTY Commuting App",
    "tagline": "Find empty subway cars",
    "description": "EMTY shows real-time passenger loads on each subway car so you can find comfortable and safe rides. I built this during a STEM Institute Entrepreneurship and App Design class, focusing on helping vulnerable commuters like elderly passengers, pregnant women, and people with disabilities who struggle most in crowded stations.\n\nThrough research at busy stations like Union Square, Times Square, and Grand Central, I discovered that these groups often can't easily move between cars once they board. EMTY would also show additional information like broken air conditioning, spills, or unpleasant smells so commuters can avoid problematic cars entirely.\n\nThe project was primarily a UI/UX design exercise in Figma, though I experimented with backend API integration. The focus was creating an interface that works for people literally running to catch trains - every design decision prioritized instant comprehension and minimal interaction.",
    "role": "I took the lead on product strategy and user experience design, spending countless hours in Figma crafting interfaces that could work in the chaos of rush hour. The real challenge was figuring out how to display real-time occupancy data in a way that's instantly understandable - even when you're sprinting down the platform. I also dove deep into the technical architecture, mapping out how live data would flow from sensors to smartphones. Beyond the design and development work, I got to pitch this concept to some pretty impressive audiences, including mentors and industry professionals who helped refine the vision.",
    "problem": "Elderly passengers, pregnant women, and people with disabilities face the biggest challenges in crowded subway systems. Unlike able-bodied commuters who can move between cars, these vulnerable groups often get stuck in uncomfortable or unsafe conditions. The information about car occupancy and conditions exists in transit systems but doesn't reach the people who need it most.",
    "approach": "My philosophy was ruthless simplicity. The interface had to work for someone who's literally running to catch a train, so every element needed to communicate instantly. I used an intuitive color system for occupancy levels - green for plenty of space, yellow for getting crowded, red for packed. The app needed to remain useful even when data connections were spotty (because subway service can be unreliable), so I built in smart caching and graceful degradation. Most importantly, I minimized the number of taps required. Open app, see train status, make decision, go - that's it.",
    "challenges": "Live data is messy. When feeds lag or drop, the UI must not flicker or mislead. We added small hints for stale data and smoothing for spikes. We kept assets light so it still feels quick.",
    "keyFeatures": [
      "Live visualization of subway car occupancy data",
      "Designed UX in Figma for intuitive data display",
      "Secure real-time backend with Firebase",
      "Pitched to the U.S. Capitol and Congressman's office"
    ],
    "techStack": "Figma, Canva, Firebase, MTA API",
    "keywords": ["UI/UX & Design"],
    "outcomes": "EMTY won the Gold Award (first place) in the STEM Institute Entrepreneurship and App Design class and later won the Congressional App Challenge in New York's 13th District. The comprehensive design system and interactive prototype demonstrated how real-time data visualization could transform urban commuting, with research showing the particular impact on vulnerable commuter populations.",
    "repoUrl": null,
    "demoUrl": "https://linktr.ee/emty7",
    "images": ["emty-1", "emty-2"],
    "galleryLinks": [
      {
        "url": "https://www.congressionalappchallenge.us/24-NY13/",
        "title": "Congressional App Challenge 2024 Winner (NY-13)",
        "ogImage": null
      }
    ],
    "startDate": "2024-08-01",
    "endDate": "2025-04-01",
    "type": "project",
     "theme": {
      "primary": "hsl(210, 40%, 50%)",
      "secondary": "hsl(160, 60%, 45%)"
    }
  },
  {
    "id": "simplify-me",
    "name": "Simplify Me.",
    "tagline": "Legal text made simple",
    "description": "Simplify Me. was born from personal frustration during SAT registration. I had a 20-minute timer to complete registration, but they required me to read their terms of service - a lengthy legal document written in complex legalese. The timer kept counting down while I struggled through pages of incomprehensible text.\n\nI built a platform that translates legal jargon into plain English summaries using the ToS;DR (Terms of Service; Didn't Read) open-source architecture combined with Google Gemini AI. Users can understand what data companies collect, how they use it, and who they share it with - all in under a minute instead of 10-15 minutes of legal reading.\n\nThe platform provides free, easy-to-understand summaries of privacy policies and terms of service for various online services and brands, making legal documents accessible to everyone.",
    "role": "I handled every aspect of this project, from the technical architecture to the user experience design. The most critical work was developing the AI prompts that guide the summarization process - these needed to be precise enough to extract accurate information while maintaining a conversational tone that doesn't intimidate users. I built the entire site infrastructure, integrating WordPress with Zapier to create a smooth content pipeline from source policies to published summaries. The copywriting was particularly important because the whole point is making legal language accessible, so every word needed to feel approachable rather than clinical or robotic.",
    "problem": "Privacy policies are long and full of legal terms. People need quick facts in normal words. They also want to know if a service is safe enough for them.",
    "approach": "Use a mix of community grades and light AI to draft a short view. Keep the claims tied to the source text. Show the main points first. Offer a link to the policy for people who want the full detail.",
    "challenges": "Working with legal documents is like walking through a minefield. One misinterpretation could mislead people about their privacy rights, which is the opposite of what I'm trying to achieve. The biggest challenge was building systems that could accurately extract key information without losing important nuances or context. I had to be extremely careful not to oversimplify to the point of being wrong. Legal language exists for precision, even if it's hard to read. I also needed to handle the fact that companies update their policies regularly - sometimes with significant changes buried in walls of text. Building automated systems to detect and highlight these updates was crucial for keeping the summaries current and trustworthy.",
    "keyFeatures": [
      "Translates complex legal jargon into plain English",
      "Summarizes key data collection and privacy policies",
      "Highlights how your data is used and shared",
      "Powered by the ToS;DR community-driven infrastructure"
    ],
    "techStack": "JavaScript, PHP, WordPress, Zapier, Google Gemini, CSS, HTML",
    "keywords": ["AI & Machine Learning", "Web Development", "Creative Tech"],
    "outcomes": "I presented Simplify Me. to my artificial intelligence class at the City College of New York. The platform reached 374 monthly active users and gained 24 registered members. Users can now understand privacy policies in under a minute instead of spending 10-15 minutes struggling through legal jargon, making informed decisions about which services align with their privacy needs.",
    "repoUrl": null,
    "demoUrl": "https://simplifyme.org",
    "images": ["simplify-me-1"],
    "startDate": "2023-01-01",
    "endDate": "2025-05-01",
    "type": "project",
    "theme": {
      "primary": "hsl(267, 95%, 48%)",
      "secondary": "hsl(14, 99%, 57%)"
    }
  },
  {
    "id": "simply",
    "name": "Simply",
    "tagline": "Chat bot explains legal docs",
    "description": "Simply is a Discord bot that I developed alongside Simplify Me. to test AI-powered document analysis in a conversational environment. The bot can process PDFs, text files, and images of any documents you don't understand, then turn complex legalese into specific, detailed, and brief explanations without missing important context.\n\nRunning on an Amazon Web Services EC2 instance, Simply saves your conversation history so you can ask follow-up questions and have natural discussions about document contents. It handles various file formats and maintains context across multiple messages, making it feel like chatting with someone who actually read and understood the fine print.\n\nThis project served as a testing ground for the AI summarization techniques that I later refined for Simplify Me. The Discord environment allowed for rapid iteration and real-time user feedback, helping me understand how people naturally want to interact with AI document analysis tools.",
    "role": "I wired the Discord API, set up the model calls, and tuned the prompts for short and honest answers. I deployed to a small cloud box and set up logging so I could watch errors and fix them fast.",
    "problem": "Reading big policies in chat is slow and not fun. People want a straight answer to a simple question. They also want to trust that the answer is tied to the source text.",
    "approach": "Keep the loop tight. Ingest the file, chunk the text, and answer with quotes. Use short follow ups to refine the result. Clear the chat on request so people control their data.",
    "challenges": "Large files push the limits. We had to guard against timeouts and rate limits. Image to text steps can fail in the wild, so we added retries and user tips. We also learned to keep answers to the point.",
    "keyFeatures": [
      "Multi-Format Support: Processes various file types, including images, PDFs, and text files.",
      "Contextual Analysis: Maintains a history window of up to 15 previous messages for coherent conversations.",
      "Privacy Control: Users can reset and clear their message history at any time.",
      "Performance and Scalability: Runs on Python in an Amazon EC2 instance for reliable performance.",
      "High-Accuracy Summaries: Extensively trained on legal texts to handle documents up to 70,000 words."
    ],
    "techStack": "Python, AWS, Google Gemini, Discord API",
    "keywords": ["AI & Machine Learning", "Creative Tech"],
    "outcomes": "We shipped a bot that handled real questions and documents from users. It proved the core idea and helped shape the rules that keep answers grounded.\nThe same engine now supports the summaries on Simplify Me., which means our early design choices continue to pay off.",
    "repoUrl": "https://github.com/aarushkmalhotra/Simply",
    "demoUrl": "https://simplifyme.org/Simply",
    "images": ["simply-1"],
    "startDate": "2024-01-01",
    "endDate": "2024-07-01",
    "type": "project",
    "theme": {
      "primary": "hsl(221, 83%, 53%)",
      "secondary": "hsl(210, 40%, 96.1%)"
    }
  },
  {
    "id": "youtube-thumbnails",
    "name": "YouTube Thumbnails",
    "tagline": "Eye-catching video covers",
    "description": "I designed all thumbnails for my YouTube channel, which peaked at 1.17k subscribers and 470,000 total views. The channel featured gaming videos, music content, and various creative projects, requiring thumbnails that could capture attention across different content types.\n\nUsing GIMP, Adobe Photoshop, and Photopea, I developed skills in advanced photo manipulation, including face morphing, shadow work, and composition techniques. I studied successful channels like MrBeast to understand effective thumbnail psychology - the use of reactions, bold colors, and visual interest factors that drive click-through rates.\n\nEach thumbnail was designed with a clear focal point and readable typography that works even on mobile screens. The goal was creating thumbnails with strong interest factors that accurately represent the content while maintaining visual consistency across the channel.",
    "role": "I plan concepts, create assets, and do all the edits. I manage a small library of styles so the channel looks consistent. I also keep notes on which ideas perform better and why.",
    "problem": "Videos compete in a busy feed. If the cover is messy, people scroll past. I needed a repeatable way to ship good covers fast.",
    "approach": "Build a simple system. Use a few type sizes and a short color set. Keep faces clear and the idea front and center. Save templates for different formats so production is fast.",
    "challenges": "It is easy to over build. I learned to stop early and keep the message simple. Exporting for different screens can break edges and text, so I run a quick check on phone and desktop.",
    "keyFeatures": [
      "Conceptualization and sketching of thumbnail ideas.",
      "Advanced photo manipulation and compositing in Photoshop.",
      "Vector asset creation with Illustrator and GIMP.",
      "Typography and color theory to maximize click-through rate.",
      "Consistent branding across channel content."
    ],
    "techStack": "Adobe Photoshop, Adobe Illustrator, GIMP",
    "keywords": ["UI/UX & Design", "Creative Tech"],
    "outcomes": "The thumbnails contributed to achieving 1.17k subscribers and 470,000 total views across gaming and music content. The designs maintained strong click-through rates while building a consistent visual brand. Skills learned in visual communication and attention-grabbing design directly influenced my approach to UI/UX work in other projects.",
    "repoUrl": null,
    "demoUrl": "https://www.youtube.com/@sunbeam20/",
    "images": ["yt-thumb-2", "yt-thumb-4", "yt-thumb-1", "yt-thumb-3"],
    "startDate": "2022-09-01",
    "endDate": "2023-08-01",
    "type": "project",
    "theme": {
      "primary": "hsl(0, 100%, 50%)",
      "secondary": "hsl(0, 0%, 13%)"
    }
  },
  {
    "id": "rvc-ui",
    "name": "RVC UI & Songs",
    "tagline": "AI voice covers",
    "description": "I wanted to hear my favorite singer, Arijit Singh, perform Bollywood songs he never actually sang. Rather than paying for expensive voice conversion apps, I built my own system using open-source Retrieval-Based Voice Conversion (RVC) technology.\n\nThe project involved training AI models on my local Windows machine through 1,200-2,000 epochs, learning the entire pipeline from audio preprocessing to final output generation. I created custom datasets, including training a model on my own voice, which taught me how crucial data quality is - more important than having the fanciest algorithms.\n\nThe most challenging part was cleaning audio data. My existing recordings had background noise and weren't optimized for training, so I had to create new datasets through extensive trial and error. The process taught me that successful AI projects depend heavily on meticulous data preparation and iterative refinement.",
    "role": "I ran the full stack from data prep to model runs to final mix. I tuned slices, removed noise, and compared settings to find a clean output. I also built a simple UI so friends could try it.",
    "problem": "People want to hear a familiar voice sing a new song, but the raw tools can be hard to use. Setups break. Files clip. Outputs sound harsh.",
    "approach": "Start with careful data work. Pick clean stems, trim silence, and normalize levels. Use moderate settings to avoid artifacts, then adjust by ear. Keep notes for each run so changes are clear.",
    "challenges": "GPU memory was tight, so I had to batch small and avoid waste. Some songs needed extra de echo steps. Matching tone between verses took extra passes.",
    "keyFeatures": [
      "Utilized Retrieval-Based Voice Conversion (RVC) for voice transformation.",
      "Leveraged pre-trained models to generate AI song covers.",
      "Created a custom voice dataset to experiment with model training.",
      "Hands-on experience with local AI model implementation."
    ],
    "techStack": "PyTorch, Gradio, Python, CUDA",
    "keywords": ["AI & Machine Learning", "Creative Tech"],
    "outcomes": "My most successful creation was an AI-generated cover of 'Ye Tune Kya Kiya' by Arijit Singh, trained with 2,000 epochs using the highest quality dataset I could compile. The final output sounds remarkably natural, demonstrating the power of patience and data quality in AI projects. The experience gave me deep insights into audio processing and machine learning that I continue to apply in other projects.",
    "repoUrl": null,
    "demoUrl": null,
    "images": ["rvc-1"],
    "startDate": "2023-07-01",
    "endDate": "2024-01-01",
    "type": "project",
    "theme": {
      "primary": "hsl(330, 80%, 55%)",
      "secondary": "hsl(20, 80%, 55%)"
    },
    "audioFiles": [
      {
        "id": "yetu-ne-kya-kiya",
        "title": "Ye Tune Kya Kiya - AI Cover",
        "file": "/arijit_1000_yetunekyakiya.mp3",
        "originalArtist": "Javed Bashir",
        "originalComposer": "Pritam",
        "originalLyricist": "Rajat Arora",
        "originalLabel": "T-Series"
      },
      {
        "id": "ajab-si",
        "title": "Ajab Si - AI Cover",
        "file": "/arijit_1100_ajabsi.mp3",
        "originalArtist": "KK",
        "originalComposer": "Vishal-Shekhar",
        "originalLyricist": "Vishal Dadlani",
        "originalLabel": "T-Series"
      }
    ]
  },
  {
    "id": "cifar-10-cnn",
    "name": "CIFAR-10 CNN",
    "tagline": "AI learns to see",
    "description": "As a Veritas AI Scholar, I worked on a group capstone project building a CNN for the CIFAR-10 dataset. This was my introduction to serious machine learning, working with 32x32 pixel images that look simple but challenge computers to classify correctly into ten categories.\n\nThe project taught me Python, TensorFlow, and Google Colab while working under the mentorship of <a href=\"https://www.linkedin.com/in/jasonjabbourj/\" target=\"_blank\" rel=\"noopener noreferrer\">Jason Jabbour</a>, a CS PhD candidate at Harvard University. I learned how CNNs and KNNs work, discovering how different architectural choices like activation functions, batch normalization, and regularization techniques impact model performance.\n\nI led the group project and coordinated our systematic experimentation approach. We achieved 72.74% test accuracy with our basic CNN, then improved to 85.9% by implementing advanced techniques like Leaky ReLU activation functions and kernel regularizations. The experience demystified deep learning and taught me the importance of methodical experimentation in AI projects.",
    "role": "I became the team's experiment coordinator, which meant keeping track of all our training runs, hyperparameter adjustments, and performance metrics. It was like being a lab manager for a machine learning research project. I spent a lot of time fine-tuning learning rates and monitoring our model's progress, making sure we weren't overfitting or missing opportunities to improve accuracy. I also took on a significant portion of the technical writing, helping document our methodology and findings in a way that other students and mentors could understand. The final presentation to our cohort was a chance to showcase not just our results, but the systematic approach we took to get there.",
    "problem": "We needed a small model that performs well on low resolution images. It had to train in a short time and give clear signals that we could analyze.",
    "approach": "We followed a disciplined experimental methodology that our mentor drilled into us: start simple, change one thing at a time, and always measure the impact. Our baseline was a basic CNN architecture, and from there we systematically tested improvements like batch normalization, dropout layers, and data augmentation techniques. The key was maintaining rigorous documentation - every experiment used the same random seed so we could isolate the effect of each change. This approach taught me that good machine learning is as much about systematic experimentation as it is about understanding the algorithms themselves.",
    "challenges": "Small images are noisy. If we made the model too big it learned the training set and missed the point. We had to use aug and regularize while watching the curve.",
    "keyFeatures": [
      "Custom Convolutional Neural Network (CNN) architecture",
      "Trained on the CIFAR-10 dataset for object recognition",
      "Led a group capstone project as a Veritas AI Scholar",
      "Mentored by a Harvard PhD candidate in Computer Science"
    ],
    "techStack": "Python, TensorFlow, CNN, Deep Learning, Google Colab",
    "keywords": ["AI & Machine Learning"],
    "outcomes": "We built a model that performs well on the test set and learned to balance capacity with generalization. The report and slides explain the trade offs in plain terms.\nMore important than a single score, the team built a habit of testing one idea at a time. That skill now shows up in later projects.",
    "repoUrl": null,
    "demoUrl": "https://docs.google.com/presentation/d/1SxtFXwkSP7j3ouTHC2i0pD7lLCfUJtWJVXgNG7_GLuo/",
    "images": [
      "cifar-1"
    ],
    "documents": [
      {
        "id": "veritas-ai-cert-11-2024",
        "title": "Veritas AI Scholar Certificate (Nov 2024)",
        "file": "/veritasai-certificate-11-2024.pdf"
      }
    ],
    "startDate": "2024-09-01",
    "endDate": "2024-11-01",
    "type": "project",
    "theme": {
      "primary": "hsl(204, 86%, 53%)",
      "secondary": "hsl(348, 100%, 61%)"
    }
  },
   {
    "id": "imdb-top-1000",
    "name": "IMDB Top 1000",
    "tagline": "My first coding project",
    "description": "My first academic programming project for AP Computer Science Principles class, built using Code.org App Lab. While most students used block coding, I chose to write actual terminal-style code to challenge myself and learn JavaScript fundamentals.\n\nThe app allows users to browse, search, and explore IMDB's top 1000 movies with features like Strict Search, Random movie selection, and a Favorites system (essentially a watchlist). This was my first experience working with external data sources and handling the complexity of processing a large dataset with 1000 entries.\n\nI learned crucial programming concepts including data handling, complexity, and runtime optimization. Initially, button clicks and movie searches were slow because I hadn't optimized the search algorithms. Learning to improve performance taught me the importance of efficient code when working with larger datasets.",
    "role": "I wrote the code, designed the screens, and debugged my way through plenty of small mistakes. I learned to read errors instead of guessing.",
    "problem": "I needed a project to learn the basics for the AP class and to prove to myself that I could build something end to end.",
    "approach": "Start small and ship features one at a time. Add search. Then add a detail card. Then add sorting. Keep the code neat and name things well.",
    "challenges": "As a beginner, I ran into all the classic programming pitfalls that every developer faces when starting out. Off-by-one errors plagued my array handling - I'd constantly find myself accessing elements that didn't exist or missing the last item in a list. State management was another beast entirely. The app would display outdated information because I hadn't properly updated variables when users performed actions. Performance was a huge learning curve too. My initial approach involved loops that searched through the entire movie database on every keystroke, which made the interface feel sluggish and unresponsive. Learning to optimize these operations and implement basic caching strategies was a game-changer for both user experience and my understanding of efficient programming.",
    "keyFeatures": [
      "Browse and search IMDB's top 1000 movies",
      "Built with vanilla JavaScript on Code.org",
      "Demonstrates fundamental data manipulation and UI skills"
    ],
    "techStack": "JavaScript, Code.org App Lab",
    "keywords": ["UI/UX & Design", "Academic"],
    "outcomes": "The project received the highest score for the Computer Science Design portion of the AP exam. My teacher showcased it to the class because of its comprehensive feature set. I personally discovered many movies through the Strict Search feature and enjoyed using the Random selection for finding new content to watch. The project gave me confidence to tackle more complex programming challenges.",
    "repoUrl": null,
    "demoUrl": "https://studio.code.org/projects/applab/w8PJdUtl1-DUKphmZ3zI9p0ZFzbPeqC5T_C0WHmYhVU",
    "images": ["imdb-top-1000-1"],
    "videoPreview": "/imdbtop1000demo.mp4",
    "hoverVideo": "/imdbtop1000demo.mp4",
    "startDate": "2023-09-01",
    "endDate": "2024-05-01",
    "type": "project",
    "theme": {
      "primary": "hsl(45, 93%, 54%)",
      "secondary": "hsl(0, 0%, 10%)"
    }
  },
  {
    "id": "album-tracks",
    "name": "Album Tracks",
    "tagline": "Custom music for videos",
    "description": "I created original 30-second montage tracks for my YouTube channel because stock music was annoying and didn't fit the vibe I wanted. These were specifically designed as upbeat EDM tracks with strong beats that content creators could use in gaming clips and edits.\n\nEach track was crafted with a strategic drop point where editors could make cuts and create dynamic scene transitions. Using SoundTrap, GarageBand, and later DaVinci Resolve for video editing, I learned the fundamentals of digital audio production and how music supports visual storytelling.\n\nThe project started when my YouTube channel began as a music-focused channel before expanding into gaming content. I wanted tracks that had the energy and structure that worked well for montages and edits, with the kind of beat drops that make viewers want to keep watching.",
    "role": "I composed, arranged, and mixed the songs. I exported clean versions and stems so I could reuse parts later.",
    "problem": "Stock music often misses the tone I want. I needed songs that fit the cut and support the story.",
    "approach": "Pick a tempo, pick a key, and build around a simple hook. Keep layers light so the track supports the video. Bounce drafts and listen on different speakers.",
    "challenges": "Balancing bass and kick on small speakers was tricky. Some DAW effects sounded great alone and messy in the full mix. I learned to trust my ears and remove parts.",
    "keyFeatures": [
      "Original tracks composed in SoundTrap and GarageBand",
      "Designed for use in video montages",
      "Exploration of digital audio workstation (DAW) software",
      "Focus on creating atmospheric and rhythmic beats"
    ],
    "techStack": "SoundTrap, GarageBand",
    "keywords": ["Creative Tech"],
    "outcomes": "The tracks contributed to my YouTube channel's growth to 1.17k subscribers and 470,000 total views. Creating original music gave me complete creative control over the audio-visual experience and eliminated concerns about copyright issues. The music production skills I learned continue to enhance my other creative and technical projects.",
    "repoUrl": null,
    "demoUrl": "https://drive.google.com/drive/folders/1DrceaX2DRv2ve6mYF7210ikESgcIUKOd",
    "images": ["album-tracks-1"],
    "videoPreview": "https://youtu.be/SWiwfNcLjWg",
    "hoverVideo": "/sunbeam-the-album-full.mp4",
    "startDate": "2022-03-01",
    "endDate": "2022-09-01",
    "type": "project",
    "theme": {
      "primary": "hsl(280, 80%, 60%)",
      "secondary": "hsl(50, 90%, 55%)"
    },
    "downloadableAudioFiles": [
      {
        "id": "edm-piano-beats",
        "title": "EDM Piano Beats",
        "file": "/edm-beats-voice.wav"
      },
      {
        "id": "edm-synth-beats",
        "title": "EDM Synth Beats",
        "file": "/edm-synth-beats.mp3"
      }
    ]
  }
]
